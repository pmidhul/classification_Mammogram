{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2nM6Iamxkvu"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import os, shutil, pathlib, glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "SEED = 4747\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "by115d_5xpv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_path_list():\n",
        "    #Create empty lists to store the paths of class 0 and class 1 images\n",
        "    class_0 = []\n",
        "    class_1 = []\n",
        "    for dir in dirs:\n",
        "        #Read class 0 and class 1 images' paths for a given patient\n",
        "        # and store them it their respective list\n",
        "        c_0 = glob.glob(f\"/content/dataset/{dir}/0/*.png\")\n",
        "        c_1 = glob.glob(f\"/content/dataset/{dir}/1/*.png\")\n",
        "\n",
        "        random.shuffle(c_0),random.shuffle(c_0)\n",
        "        random.shuffle(c_1),random.shuffle(c_1)\n",
        "\n",
        "        #Add the class 0 and class 1 images' paths for a given patient\n",
        "        # to the main list\n",
        "        class_0.extend(c_0)\n",
        "        class_1.extend(c_1)\n",
        "\n",
        "        random.shuffle(class_0),random.shuffle(class_0)\n",
        "        random.shuffle(class_1),random.shuffle(class_1)\n",
        "\n",
        "    #Shuffle the paths lists randomly\n",
        "    random.shuffle(class_0), random.shuffle(class_0)\n",
        "    random.shuffle(class_1), random.shuffle(class_1)\n",
        "\n",
        "    #Return the class_0 and class_1 lists\n",
        "    return class_0, class_1"
      ],
      "metadata": {
        "id": "VConVCsextwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(train_split=0.85):\n",
        "    #Use the \"get_image_path_list\" function to get two separate lists\n",
        "    # of all the images of class 0 and class 1\n",
        "    class0, class1 = get_image_path_list()\n",
        "\n",
        "    #Shuffle the paths lists randomly\n",
        "    random.shuffle(class0), random.shuffle(class0)\n",
        "    random.shuffle(class1), random.shuffle(class1)\n",
        "\n",
        "    #Calculate the total number of images of both classes\n",
        "    total_img0 = len(class0)\n",
        "    total_img1 = len(class1)\n",
        "\n",
        "    #Calculate the number of images for train dataset for both classes\n",
        "    train0_thresh = int(total_img0 * train_split)\n",
        "    train1_thresh = int(total_img1 * train_split)\n",
        "\n",
        "    #Create sub-directories for train directory\n",
        "    train0_dir = \"/content/new_dataset/train/0\"\n",
        "    train1_dir = \"/content/new_dataset/train/1\"\n",
        "    os.makedirs(train0_dir)\n",
        "    os.mkdir(train1_dir)\n",
        "\n",
        "    #Create sub-directories for test directory\n",
        "    test0_dir = \"/content/new_dataset/test/0\"\n",
        "    test1_dir = \"/content/new_dataset/test/1\"\n",
        "    os.makedirs(test0_dir)\n",
        "    os.mkdir(test1_dir)\n",
        "\n",
        "    #Let's copy the image of class 0\n",
        "    for i in range(total_img0):\n",
        "        path = class0[i]\n",
        "        #If the index of the current image is less then the\n",
        "        # validation threshold then, we'll copy the image to\n",
        "        # the train directory, otherwise to the test directory\n",
        "        if i < train0_thresh:\n",
        "            shutil.copy(src=path, dst= train0_dir)\n",
        "        else:\n",
        "            shutil.copy(src=path, dst= test0_dir)\n",
        "\n",
        "        #Let's copy the image of class 1\n",
        "\n",
        "    for i in range(total_img1):\n",
        "        path = class1[i]\n",
        "        #If the index of the current image is less then the\n",
        "        # validation threshold then, we'll copy the image to\n",
        "        # the train directory, otherwise to the test directory\n",
        "        if i < train1_thresh:\n",
        "            shutil.copy(src=path, dst= train1_dir)\n",
        "        else:\n",
        "            shutil.copy(src=path, dst= test1_dir)"
      ],
      "metadata": {
        "id": "_0W898wNyDT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_dataset(train_split=0.80)"
      ],
      "metadata": {
        "id": "AFAAgJJWyGYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class0, class1 = get_image_path_list()\n",
        "\n",
        "print(\"Size of 80% images of class 0 =\", int(len(class0)*0.80))\n",
        "print(\"Size of 80% images of class 1 =\", int(len(class1)*0.80))\n",
        "print(\"Size of 80% images of class both classes =\", int(len(class1)*0.80) + int(len(class0)*0.80))\n",
        "\n",
        "train_data_size = glob.glob(\"/content/new_dataset/train/**/*.png\")\n",
        "print(\"Size of training dataset =\",len(train_data_size))"
      ],
      "metadata": {
        "id": "xjNnRgBRyInu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (50,50)\n",
        "batch_size = 32\n",
        "\n",
        "#Get \"tensorflow.data.Dataset\" object for the training data\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    \"/content/new_dataset/train\",\n",
        "    labels = \"inferred\",\n",
        "    label_mode = 'int',\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=SEED,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "#Get \"tensorflow.data.Dataset\" object for the validation data\n",
        "validation_dataset =image_dataset_from_directory(\n",
        "    \"/content/new_dataset/train\",\n",
        "    labels = \"inferred\",\n",
        "    label_mode = 'int',\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=SEED,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "18obqDWLyLsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get \"tensorflow.data.Dataset\" object for the test data\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    \"/content/new_dataset/test\",\n",
        "    labels = \"inferred\",\n",
        "    label_mode = 'int',\n",
        "    seed=SEED,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "x_IECctoyOzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Build"
      ],
      "metadata": {
        "id": "Lsr58YYIyUlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.3),\n",
        "        layers.RandomZoom(0.3)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "v4uFDfidyXKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(50, 50, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "#Since the image size is very small (50x50), so we are starting with\n",
        "# large number for \"filters\". Usually, we start with small value\n",
        "# and gradually increase. But here, we are diverging from\n",
        "# our normal architecture due to small image size. Because,\n",
        "# after the first layer, our image will reduce to (24x24)\n",
        "# and most of the information in the image will be lost.\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(512, activation=\"relu\")(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "we3jrjiJyZzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "GFWPsLt-ydbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model summary to a text file\n",
        "with open('model_summary.txt', 'w') as f:\n",
        "    model.summary(print_fn=lambda x: f.write(x + '\\n'))"
      ],
      "metadata": {
        "id": "W9sCv7JSygJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "               optimizer=\"rmsprop\",\n",
        "               metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "za-7St1Fyg0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "            #keras.callbacks.EarlyStopping(monitor='val_loss',patience=5),\n",
        "            keras.callbacks.ModelCheckpoint(filepath=\"CanDetect.keras\",\n",
        "                                            save_best_only=True,\n",
        "                                            monitor=\"val_loss\")\n",
        "            ]"
      ],
      "metadata": {
        "id": "n86f0R8QyjfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "                    train_dataset,\n",
        "                    epochs=20,\n",
        "                    validation_data=validation_dataset,\n",
        "                    callbacks=callbacks\n",
        "                   )"
      ],
      "metadata": {
        "id": "hSaWVvAkylTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "fig, ax = plt.subplots(1,2,figsize=(12,8))\n",
        "\n",
        "ax[0].plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "ax[0].plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "ax[0].set_title(\"Training and validation accuracy\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "ax[1].plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "ax[1].set_title(\"Training and validation loss\")\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "FjEVpwsvyl8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = keras.models.load_model(\"/content/CanDetect.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "QMxyKaQIyoe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_the_model():\n",
        "  class_true = []\n",
        "  class_pred = []\n",
        "\n",
        "  for batch_data, batch_labels in test_dataset:\n",
        "\n",
        "      #Make predictions\n",
        "      pred_labels = test_model.predict(batch_data)\n",
        "\n",
        "      #Convert the NumPy ndarray object to simple list\n",
        "      pred_labels_np = [float(i) for i in pred_labels]\n",
        "\n",
        "      # Convert \"tf.data.Dataset\" to NumPy array\n",
        "      batch_labels_np = tfds.as_numpy(batch_labels)\n",
        "\n",
        "      #Append the true and predicted labels to their respective list\n",
        "      class_true.extend(batch_labels_np)\n",
        "      class_pred.extend(pred_labels_np)\n",
        "  return class_true, class_pred"
      ],
      "metadata": {
        "id": "K9vxIIsUysF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_true, class_pred = test_the_model()"
      ],
      "metadata": {
        "id": "rG_neS6TytqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_pred_int = [round(i) for i in class_pred]"
      ],
      "metadata": {
        "id": "Qf_HMeZIy073"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion = confusion_matrix(class_true, class_pred_int)\n",
        "fig,ax = plt.subplots(figsize=(8,5))\n",
        "sns.heatmap(confusion, annot=True, linewidths=0.1,\n",
        "            cmap=\"BuGn\", linecolor=\"green\", fmt= '.1f', ax=ax)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U3oNGViXy1de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve"
      ],
      "metadata": {
        "id": "HIfhYMkay6IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROC_AUC = roc_auc_score(class_true, class_pred_int)\n",
        "# summarize scores\n",
        "print('ROC AUC score =%.3f' % (ROC_AUC))\n",
        "\n",
        "#Calculate roc curves\n",
        "fpr, tpr, thresholds = roc_curve(class_true, class_pred_int)\n",
        "\n",
        "#Plot the roc curve\n",
        "plt.plot(fpr, tpr,label=\"AUC=\"+str(ROC_AUC))\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')"
      ],
      "metadata": {
        "id": "f8AY7PyFy7oF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}